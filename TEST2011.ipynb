{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5db413e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to read JSON with multiline=true...\n",
      "\n",
      "--- Schema Structure ---\n",
      "root\n",
      " |-- address: struct (nullable = true)\n",
      " |    |-- city: string (nullable = true)\n",
      " |    |-- country: string (nullable = true)\n",
      " |    |-- state: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- employee_id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- skills: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "\n",
      "--- Data Preview ---\n",
      "+-----------------------------+---+----------------+-----------+-------------------+---------------------------------+\n",
      "|address                      |age|department      |employee_id|name               |skills                           |\n",
      "+-----------------------------+---+----------------+-----------+-------------------+---------------------------------+\n",
      "|{Kolkata, India, West Bengal}|28 |Data Engineering|101        |Somnath Chakraborty|[Python, Azure, PySpark]         |\n",
      "|{Bangalore, India, Karnataka}|26 |Data Science    |102        |XYZ                |[Machine Learning, SQL, Power BI]|\n",
      "|{Hyderabad, India, Telangana}|29 |AI Research     |103        |Noorjahan Ali      |[Deep Learning, NLP, Python]     |\n",
      "+-----------------------------+---+----------------+-----------+-------------------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def check_bronze_schema_safe():\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Check Bronze Schema Debug\") \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.4.1,com.amazonaws:aws-java-sdk-bundle:1.12.772\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://127.0.0.1:9000\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "    path = \"s3a://bronze/\"\n",
    "    print(\"Attempting to read JSON with multiline=true...\")\n",
    "\n",
    "    try:\n",
    "        # FIX: Added multiline=true\n",
    "        df = spark.read \\\n",
    "            .option(\"inferSchema\", \"true\") \\\n",
    "            .option(\"multiline\", \"true\") \\\n",
    "            .json(path)\n",
    "            \n",
    "        print(\"\\n--- Schema Structure ---\")\n",
    "        df.printSchema()\n",
    "\n",
    "        print(\"\\n--- Data Preview ---\")\n",
    "        df.show(5, truncate=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError reading JSON: {e}\")\n",
    "        print(\"\\n--- DEBUGGING: RAW FILE CONTENT ---\")\n",
    "        # If JSON fails, let's read it as simple text to see what the file actually looks like\n",
    "        try:\n",
    "            df_text = spark.read.text(path)\n",
    "            print(\"Here is what the raw file looks like (first 10 lines):\")\n",
    "            df_text.show(10, truncate=False)\n",
    "        except Exception as text_e:\n",
    "            print(f\"Could not read as text either: {text_e}\")\n",
    "\n",
    "    finally:\n",
    "        spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    check_bronze_schema_safe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baca086b",
   "metadata": {},
   "source": [
    "#STOP_THE SESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2227e45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Error: 'spark' object not found. Ensure your SparkSession was initialized.\n"
     ]
    }
   ],
   "source": [
    "# The print statement is indented once (4 spaces)\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"✅ SparkSession successfully stopped.\")\n",
    "except NameError:\n",
    "    print(\"⚠️ Error: 'spark' object not found. Ensure your SparkSession was initialized.\")\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e576185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing Basic Spark Session ---\n",
      "\n",
      "✅ Detected Spark Version: 4.0.1\n",
      "\n",
      "--- Compatibility Guidance ---\n",
      "Please check the official Delta Lake documentation for the package corresponding to this specific Spark version.\n",
      "\n",
      "--- Full Build Info (may contain Scala version) ---\n",
      "-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-modules=jdk.incubator.vector --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dio.netty.tryReflectionSetAccessible=true\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "def check_version():\n",
    "    \"\"\"\n",
    "    Initializes a basic Spark session and prints the Spark version \n",
    "    to help diagnose Scala version compatibility issues with Delta Lake.\n",
    "    \"\"\"\n",
    "    print(\"--- Initializing Basic Spark Session ---\")\n",
    "\n",
    "    try:\n",
    "        # Create a SparkSession without any specific external packages\n",
    "        spark = SparkSession.builder \\\n",
    "            .appName(\"VersionChecker\") \\\n",
    "            .getOrCreate()\n",
    "        \n",
    "        # Set log level to reduce console clutter\n",
    "        spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "        # 1. Print the official Spark version\n",
    "        spark_version = spark.version\n",
    "        print(f\"\\n✅ Detected Spark Version: {spark_version}\")\n",
    "        \n",
    "        # 2. Infer the likely Scala compatibility\n",
    "        # Spark 3.3 and older often use Scala 2.12. Spark 3.4+ often uses Scala 2.12 or 2.13.\n",
    "        print(\"\\n--- Compatibility Guidance ---\")\n",
    "        if spark_version.startswith(\"3.4\") or spark_version.startswith(\"3.5\"):\n",
    "            print(\"Spark 3.4/3.5 can be built with Scala 2.12 or 2.13. You likely need `_2.12` or `_2.13` in your Delta package name.\")\n",
    "        elif spark_version.startswith(\"3.\"):\n",
    "             print(\"Spark 3.x (older than 3.4) is typically built with Scala 2.12. Try `io.delta:delta-spark_2.12:2.4.0`.\")\n",
    "        else:\n",
    "             print(\"Please check the official Delta Lake documentation for the package corresponding to this specific Spark version.\")\n",
    "\n",
    "        print(\"\\n--- Full Build Info (may contain Scala version) ---\")\n",
    "        # This often reveals the specific build details including Scala version\n",
    "        print(spark.sparkContext.getConf().get('spark.executor.extraJavaOptions'))\n",
    "        \n",
    "        spark.stop()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Failed to initialize Spark or get version info: {e}\")\n",
    "        print(\"Ensure PySpark is correctly installed and configured.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    check_version()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
