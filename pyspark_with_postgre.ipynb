{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f599bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Setting up SparkSession...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /home/somnath/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /home/somnath/.ivy2.5.2/jars\n",
      "org.postgresql#postgresql added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-27dc6d01-1344-454a-93b0-d7c9badf497d;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.postgresql#postgresql;42.5.0 in central\n",
      "\tfound org.checkerframework#checker-qual;3.5.0 in central\n",
      ":: resolution report :: resolve 181ms :: artifacts dl 7ms\n",
      "\t:: modules in use:\n",
      "\torg.checkerframework#checker-qual;3.5.0 from central in [default]\n",
      "\torg.postgresql#postgresql;42.5.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-27dc6d01-1344-454a-93b0-d7c9badf497d\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/7ms)\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...SparkSession created successfully with JDBC driver.\n",
      "\n",
      "Sample DataFrame to write to Postgres:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|key|value|\n",
      "+---+-----+\n",
      "|  A|   10|\n",
      "|  B|   20|\n",
      "|  C|   30|\n",
      "+---+-----+\n",
      "\n",
      "\n",
      "Writing data to PostgreSQL table 'sample_data'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Data successfully written.\n",
      "\n",
      "Reading data back from PostgreSQL table 'sample_data'...\n",
      "...Data successfully read and verified:\n",
      "+---+-----+\n",
      "|key|value|\n",
      "+---+-----+\n",
      "|  B|   20|\n",
      "|  A|   10|\n",
      "|  C|   30|\n",
      "+---+-----+\n",
      "\n",
      "\n",
      "PySpark-PostgreSQL Integration Test: SUCCESS!\n",
      "\n",
      "Spark Session stopped.\n"
     ]
    }
   ],
   "source": [
    "# --- JUPYTER NOTEBOOK CELL ---\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import sys\n",
    "\n",
    "# --- Configuration for JDBC Driver ---\n",
    "# We use the Maven coordinates for the PostgreSQL JDBC driver.\n",
    "POSTGRES_JDBC_PACKAGE = \"org.postgresql:postgresql:42.5.0\"\n",
    "\n",
    "# --- 1. Define Connection Parameters ---\n",
    "# Using the successful credentials: DB=spark_db, User=spark_user, Pass=1110897\n",
    "POSTGRES_URL = \"jdbc:postgresql://localhost:5432/spark_db\"\n",
    "POSTGRES_PROPERTIES = {\n",
    "    \"user\": \"spark_user\",      \n",
    "    \"password\": \"1110897\",     \n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "TABLE_NAME = \"sample_data\"\n",
    "\n",
    "try:\n",
    "    print(\"1. Setting up SparkSession...\")\n",
    "    \n",
    "    # *** CRUCIAL CHANGE FOR NOTEBOOK ***\n",
    "    # Pass the JDBC package directly to the SparkSession builder\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"PostgresIntegration_Jupyter\") \\\n",
    "        .config(\"spark.jars.packages\", POSTGRES_JDBC_PACKAGE) \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    print(\"...SparkSession created successfully with JDBC driver.\")\n",
    "    \n",
    "    # --- 2. Create a Sample DataFrame ---\n",
    "    data = [(\"A\", 10), (\"B\", 20), (\"C\", 30)]\n",
    "    columns = [\"key\", \"value\"]\n",
    "    source_df = spark.createDataFrame(data, columns)\n",
    "    print(\"\\nSample DataFrame to write to Postgres:\")\n",
    "    source_df.show()\n",
    "\n",
    "    # --- 3. Write Data to PostgreSQL (Load) ---\n",
    "    print(f\"\\nWriting data to PostgreSQL table '{TABLE_NAME}'...\")\n",
    "    source_df.write \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", POSTGRES_URL) \\\n",
    "        .option(\"dbtable\", TABLE_NAME) \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .options(**POSTGRES_PROPERTIES) \\\n",
    "        .save()\n",
    "    print(\"...Data successfully written.\")\n",
    "\n",
    "    # --- 4. Read Data from PostgreSQL (Extract) ---\n",
    "    print(f\"\\nReading data back from PostgreSQL table '{TABLE_NAME}'...\")\n",
    "    read_df = spark.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", POSTGRES_URL) \\\n",
    "        .option(\"dbtable\", TABLE_NAME) \\\n",
    "        .options(**POSTGRES_PROPERTIES) \\\n",
    "        .load()\n",
    "\n",
    "    print(\"...Data successfully read and verified:\")\n",
    "    read_df.show()\n",
    "    print(\"\\nPySpark-PostgreSQL Integration Test: SUCCESS!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nPySpark-PostgreSQL Integration Test: FAILED!\")\n",
    "    # Print the specific error for debugging\n",
    "    print(f\"Error: {e}\")\n",
    "    # This ensures a clean exit in a terminal, though less critical in a notebook\n",
    "    sys.exit(1)\n",
    "\n",
    "finally:\n",
    "    # Stop the Spark Session\n",
    "    if 'spark' in locals() and spark is not None:\n",
    "        spark.stop()\n",
    "        print(\"\\nSpark Session stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
