{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34ed9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "1. INITIAL DATAFRAME\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|Alice| 34|\n",
      "|  Bob| 45|\n",
      "|Cathy| 29|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"ExampleApp\").master(\"local[*]\").getOrCreate()\n",
    "print(\"=\" * 50)\n",
    "print(\"1. INITIAL DATAFRAME\")\n",
    "print(\"=\" * 50)\n",
    "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Cathy\", 29)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "df.show()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0426f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "APPROACH 1: RDD (Low-Level / Unoptimized)\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered RDD: [('Alice', 34), ('Bob', 45), ('David', 34)]\n",
      "Average Age (RDD): 37.666666666666664\n",
      "\n",
      "==================================================\n",
      "APPROACH 2: DATAFRAME (High-Level / Optimized)\n",
      "==================================================\n",
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|Alice| 34|\n",
      "|  Bob| 45|\n",
      "|David| 34|\n",
      "+-----+---+\n",
      "\n",
      "+------------------+\n",
      "|       Average_Age|\n",
      "+------------------+\n",
      "|37.666666666666664|\n",
      "+------------------+\n",
      "\n",
      "Optimized Logical Plan:\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[], functions=[avg(Age#28L)])\n",
      "   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=112]\n",
      "      +- HashAggregate(keys=[], functions=[partial_avg(Age#28L)])\n",
      "         +- Project [Age#28L]\n",
      "            +- Filter (isnotnull(Age#28L) AND (Age#28L > 30))\n",
      "               +- Scan ExistingRDD[Name#27,Age#28L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1. WSL/Linux Environment Setup\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "def main():\n",
    "    # Initialize SparkSession\n",
    "    # We add 'config' settings to ensure WSL networking works smoothly\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"RDD_vs_DataFrame\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.driver.host\", \"127.0.0.1\") \\\n",
    "        .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Sample Data\n",
    "    data = [(\"Alice\", 34), (\"Bob\", 45), (\"Cathy\", 29), (\"David\", 34), (\"Eve\", 25)]\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"APPROACH 1: RDD (Low-Level / Unoptimized)\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Create RDD\n",
    "    rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "    # Task: Filter Age > 30 and find the average\n",
    "    # RDD requires manual mapping and reducing\n",
    "    filtered_rdd = rdd.filter(lambda x: x[1] > 30)\n",
    "    \n",
    "    # map(lambda x: (sum_value, count_value))\n",
    "    counts = filtered_rdd.map(lambda x: (x[1], 1)) \\\n",
    "                         .reduce(lambda a, b: (a[0] + b[0], a[1] + b[1]))\n",
    "\n",
    "    avg_age_rdd = counts[0] / counts[1]\n",
    "    print(f\"Filtered RDD: {filtered_rdd.collect()}\")\n",
    "    print(f\"Average Age (RDD): {avg_age_rdd}\")\n",
    "\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"APPROACH 2: DATAFRAME (High-Level / Optimized)\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "\n",
    "    # Task: Same filter and average\n",
    "    # DataFrame uses the Catalyst Optimizer to prune columns and optimize the plan\n",
    "    df_result = df.filter(df.Age > 30).select(F.avg(\"Age\").alias(\"Average_Age\"))\n",
    "\n",
    "    df.filter(df.Age > 30).show()\n",
    "    df_result.show()\n",
    "\n",
    "    # Bonus: View the Physical Plan (Interviewers love this!)\n",
    "    print(\"Optimized Logical Plan:\")\n",
    "    df_result.explain()\n",
    "\n",
    "    spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a99278b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/12/26 04:46:41 WARN Utils: Your hostname, Somnath, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/12/26 04:46:41 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/26 04:46:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "UPDATED DATAFRAME\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|Alice| 34|\n",
      "|  Bob| 45|\n",
      "|Cathy| 29|\n",
      "|David| 22|\n",
      "|  Eve| 31|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "    .appName(\"ExampleApp\")\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 1. Original Data\n",
    "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Cathy\", 29)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "\n",
    "# 2. New Data\n",
    "new_data = [(\"David\", 22), (\"Eve\", 31)]\n",
    "new_df = spark.createDataFrame(new_data, [\"Name\", \"Age\"])\n",
    "\n",
    "# 3. Combine them\n",
    "df_combined = df.union(new_df)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"UPDATED DATAFRAME\")\n",
    "print(\"=\" * 50)\n",
    "df_combined.show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbf3151",
   "metadata": {},
   "source": [
    "# FILE WILL SAVE IN OUTPUT_DATA FOLDER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2644f162",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved to output_data!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# FORCE LOCAL ENVIRONMENT\n",
    "#os.environ['PYSPARK_PYTHON'] = '/home/somnath/all_env/pyspark-kafka-env-venv/bin/python'\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ExampleApp\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"file:///\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "try:\n",
    "    data = [(\"Alice\", 34), (\"Bob\", 45)]\n",
    "    df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "\n",
    "    # Path must start with file:// for some Spark/Hadoop versions on Linux\n",
    "    output_path = \"file:///home/somnath/my_vscode_project/output_data\"\n",
    "    \n",
    "    # Use standard save\n",
    "    df.write.mode(\"overwrite\").csv(output_path, header=True)\n",
    "    print(\"Successfully saved to output_data!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Detailed Error: {e}\")\n",
    "\n",
    "finally:\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbe1abfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "SUCCESS!\n",
      "File created at: /home/somnath/my_vscode_project/output_data/final_processed_data.csv\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1. SETUP\n",
    "#os.environ['PYSPARK_PYTHON'] = '/home/somnath/all_env/pyspark-kafka-env-venv/bin/python'\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CustomNaming\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"file:///\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "try:\n",
    "    # 2. CREATE DATA\n",
    "    data = [(\"Alice\", \"Sales\", 4500), (\"Bob\", \"Sales\", 3000), (\"Cathy\", \"IT\", 6000)]\n",
    "    df = spark.createDataFrame(data, [\"Name\", \"Dept\", \"Salary\"])\n",
    "\n",
    "    # 3. PATHS\n",
    "    base_dir = \"/home/somnath/my_vscode_project/output_data\"\n",
    "    temp_dir = os.path.join(base_dir, \"temp_spark_out\")\n",
    "    final_file_name = \"final_processed_data.csv\"\n",
    "    final_path = os.path.join(base_dir, final_file_name)\n",
    "\n",
    "    # Clean up old data\n",
    "    if os.path.exists(base_dir):\n",
    "        shutil.rmtree(base_dir)\n",
    "    os.makedirs(base_dir)\n",
    "\n",
    "    # 4. SAVE TO TEMP FOLDER\n",
    "    # We use coalesce(1) to ensure only ONE part file is created\n",
    "    df.coalesce(1).write.mode(\"overwrite\").csv(temp_dir, header=True)\n",
    "\n",
    "    # 5. RENAME THE PART FILE TO YOUR CODE NAME\n",
    "    # Find the file that starts with 'part-' inside the temp folder\n",
    "    part_file = [f for f in os.listdir(temp_dir) if f.startswith(\"part-\") and f.endswith(\".csv\")][0]\n",
    "    \n",
    "    # Move and rename it to the main output_data folder\n",
    "    shutil.move(os.path.join(temp_dir, part_file), final_path)\n",
    "\n",
    "    # 6. CLEAN UP\n",
    "    shutil.rmtree(temp_dir)\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"SUCCESS!\")\n",
    "    print(f\"File created at: {final_path}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "finally:\n",
    "    spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
