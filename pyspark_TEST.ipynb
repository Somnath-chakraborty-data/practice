{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a34ed9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/12/25 06:18:16 WARN Utils: Your hostname, Somnath, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/12/25 06:18:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/25 06:18:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "1. INITIAL DATAFRAME\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|Alice| 34|\n",
      "|  Bob| 45|\n",
      "|Cathy| 29|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"ExampleApp\").master(\"local[*]\").getOrCreate()\n",
    "print(\"=\" * 50)\n",
    "print(\"1. INITIAL DATAFRAME\")\n",
    "print(\"=\" * 50)\n",
    "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Cathy\", 29)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "df.show()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a99278b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ExampleApp\").master(\"local[*]\").getOrCreate()\n",
    "\n",
    "# 1. Original Data\n",
    "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Cathy\", 29)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "\n",
    "# 2. New Data\n",
    "new_data = [(\"David\", 22), (\"Eve\", 31)]\n",
    "new_df = spark.createDataFrame(new_data, [\"Name\", \"Age\"])\n",
    "\n",
    "# 3. Combine them\n",
    "df_combined = df.union(new_df)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"UPDATED DATAFRAME\")\n",
    "print(\"=\" * 50)\n",
    "df_combined.show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbf3151",
   "metadata": {},
   "source": [
    "# FILE WILL SAVE IN OUTPUT_DATA FOLDER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2644f162",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved to output_data!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# FORCE LOCAL ENVIRONMENT\n",
    "os.environ['PYSPARK_PYTHON'] = '/home/somnath/all_env/pyspark-kafka-env-venv/bin/python'\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ExampleApp\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"file:///\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "try:\n",
    "    data = [(\"Alice\", 34), (\"Bob\", 45)]\n",
    "    df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "\n",
    "    # Path must start with file:// for some Spark/Hadoop versions on Linux\n",
    "    output_path = \"file:///home/somnath/my_vscode_project/output_data\"\n",
    "    \n",
    "    # Use standard save\n",
    "    df.write.mode(\"overwrite\").csv(output_path, header=True)\n",
    "    print(\"Successfully saved to output_data!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Detailed Error: {e}\")\n",
    "\n",
    "finally:\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe1abfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "SUCCESS!\n",
      "File created at: /home/somnath/my_vscode_project/output_data/final_processed_data.csv\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1. SETUP\n",
    "os.environ['PYSPARK_PYTHON'] = '/home/somnath/all_env/pyspark-kafka-env-venv/bin/python'\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CustomNaming\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"file:///\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "try:\n",
    "    # 2. CREATE DATA\n",
    "    data = [(\"Alice\", \"Sales\", 4500), (\"Bob\", \"Sales\", 3000), (\"Cathy\", \"IT\", 6000)]\n",
    "    df = spark.createDataFrame(data, [\"Name\", \"Dept\", \"Salary\"])\n",
    "\n",
    "    # 3. PATHS\n",
    "    base_dir = \"/home/somnath/my_vscode_project/output_data\"\n",
    "    temp_dir = os.path.join(base_dir, \"temp_spark_out\")\n",
    "    final_file_name = \"final_processed_data.csv\"\n",
    "    final_path = os.path.join(base_dir, final_file_name)\n",
    "\n",
    "    # Clean up old data\n",
    "    if os.path.exists(base_dir):\n",
    "        shutil.rmtree(base_dir)\n",
    "    os.makedirs(base_dir)\n",
    "\n",
    "    # 4. SAVE TO TEMP FOLDER\n",
    "    # We use coalesce(1) to ensure only ONE part file is created\n",
    "    df.coalesce(1).write.mode(\"overwrite\").csv(temp_dir, header=True)\n",
    "\n",
    "    # 5. RENAME THE PART FILE TO YOUR CODE NAME\n",
    "    # Find the file that starts with 'part-' inside the temp folder\n",
    "    part_file = [f for f in os.listdir(temp_dir) if f.startswith(\"part-\") and f.endswith(\".csv\")][0]\n",
    "    \n",
    "    # Move and rename it to the main output_data folder\n",
    "    shutil.move(os.path.join(temp_dir, part_file), final_path)\n",
    "\n",
    "    # 6. CLEAN UP\n",
    "    shutil.rmtree(temp_dir)\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"SUCCESS!\")\n",
    "    print(f\"File created at: {final_path}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "finally:\n",
    "    spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
